{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde0b813",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook outlines the essential data preparation steps required for subsequent data analysis and model training. The dataset consists of 38 individual datasets, each representing one day of machine data. All datasets are concatenated into a single dataset, which is then shuffled and split into two subsets: 80% for training and 20% for testing.\n",
    "\n",
    "Later, the training subset is further split into training and validation sets using an 80/20 ratio. This results in 64% of the overall data being used for model training, 16% for validation, and 20% for testing.\n",
    "\n",
    "The data preparation process includes the following key steps:\n",
    "1. **Data Cleaning:** Removing unnecessary columns, handling missing or invalid values, and converting numeric columns to the correct data types.\n",
    "2. **Preserving Timestamps:** The `_time` column is kept in the concatenated dataset for future analysis but is removed from the training and testing datasets to avoid interference during model training.\n",
    "3. **Label Encoding:** Converting machine state labels into numerical values for compatibility with machine learning models.\n",
    "4. **Feature Normalization**\n",
    "5. **Oversampling with SMOTE**\n",
    "\n",
    "This pipeline ensures that the data is cleaned, normalized, and prepared in a consistent manner for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951616f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import joblib\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5ec306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 38 files with 2883832 rows in total.\n"
     ]
    }
   ],
   "source": [
    "# Load and concatenate datasets\n",
    "\n",
    "print(\"Loading data...\")\n",
    "folder_path = 'labeled_data'\n",
    "all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dataframes = [pd.read_csv(file, sep=\";\", low_memory=False) for file in all_files]\n",
    "concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"Loaded {len(dataframes)} files with {concatenated_df.shape[0]} rows in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f2da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "Cleaned data has 1346830 rows and 44 columns.\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "concatenated_df = concatenated_df.drop(columns=['Unnamed: 0', 'value'], errors='ignore')\n",
    "concatenated_df = concatenated_df.loc[concatenated_df['Label'] != '0']\n",
    "concatenated_df = concatenated_df.replace(',', '.', regex=True)\n",
    "columns_to_convert = [col for col in concatenated_df.columns if col not in ['Label', '_time']]\n",
    "concatenated_df[columns_to_convert] = concatenated_df[columns_to_convert].astype(float)\n",
    "if '_time' in concatenated_df.columns:\n",
    "    concatenated_df['_time'] = pd.to_datetime(concatenated_df['_time'])\n",
    "print(f\"Cleaned data has {concatenated_df.shape[0]} rows and {concatenated_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d078972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates...\n",
      "Warning: 436308 duplicate rows found (ignoring the '_time' column).\n",
      "After removing duplicates, data has 968717 rows and 44 columns.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates (ignoring the '_time' column)\n",
    "\n",
    "print(\"Checking for duplicates...\")\n",
    "columns_to_check = [col for col in concatenated_df.columns if col != '_time']\n",
    "duplicate_rows = concatenated_df.duplicated(subset=columns_to_check, keep=False)\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"Warning: {num_duplicates} duplicate rows found (ignoring the '_time' column).\")\n",
    "else:\n",
    "    print(\"No duplicate rows found (ignoring the '_time' column).\")\n",
    "\n",
    "# Remove duplicates ignoring the '_time' column\n",
    "concatenated_df = concatenated_df.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "# Output the new number of rows and columns\n",
    "print(f\"After removing duplicates, data has {concatenated_df.shape[0]} rows and {concatenated_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286a94fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels...\n",
      "Label encoding completed. Mapping:\n",
      "  Block einladen: 0\n",
      "  Seitenbesäumung: 1\n",
      "  Kleben: 2\n",
      "  Schopfbesäumung: 3\n",
      "  Produktion: 4\n",
      "  Bodenhaut entfernen: 5\n",
      "  Stillstand: 6\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "\n",
    "print(\"Encoding labels...\")\n",
    "unique_labels = concatenated_df['Label'].unique()\n",
    "replace_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "concatenated_df['Label'] = concatenated_df['Label'].replace(replace_dict)\n",
    "print(\"Label encoding completed. Mapping:\")\n",
    "for label, idx in replace_dict.items():\n",
    "    print(f\"  {label}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132341be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "Training set: 774973 rows\n",
      "Test set: 193744 rows\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "\n",
    "random_indices = np.random.RandomState(seed=42).permutation(len(concatenated_df.index))\n",
    "shuffled_df = concatenated_df.iloc[random_indices]\n",
    "split_index = int(len(shuffled_df) * 0.8)\n",
    "\n",
    "train_df = shuffled_df.iloc[:split_index].copy()\n",
    "test_df = shuffled_df.iloc[split_index:].copy()\n",
    "print(f\"Training set: {len(train_df)} rows\\nTest set: {len(test_df)} rows\")\n",
    "\n",
    "# Remove '_time' from training and test datasets\n",
    "if '_time' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['_time'])\n",
    "if '_time' in test_df.columns:\n",
    "    test_df = test_df.drop(columns=['_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7795acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splitting is consistent.\n",
      "No common rows between training and test datasets.\n",
      "Label encoding is consistent between training and test datasets.\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "\n",
    "# Verify dataset splitting\n",
    "total_rows = len(concatenated_df)\n",
    "split_rows = len(train_df) + len(test_df)\n",
    "if total_rows == split_rows:\n",
    "    print(\"Dataset splitting is consistent.\")\n",
    "else:\n",
    "    print(f\"Mismatch in dataset splitting: Total={total_rows}, Train+Test={split_rows}\")\n",
    "\n",
    "# Check for duplicate rows between training and test datasets\n",
    "train_hashes = pd.util.hash_pandas_object(train_df, index=False)\n",
    "test_hashes = pd.util.hash_pandas_object(test_df, index=False)\n",
    "\n",
    "# Prüfen auf gemeinsame Hashes\n",
    "common_hashes = set(train_hashes).intersection(set(test_hashes))\n",
    "if common_hashes:\n",
    "    print(f\"Warning: {len(common_hashes)} common rows found between training and test datasets.\")\n",
    "else:\n",
    "    print(\"No common rows between training and test datasets.\")\n",
    "    \n",
    "# Verify label consistency between training and test datasets\n",
    "missing_labels = set(test_df['Label']) - set(train_df['Label'])\n",
    "if missing_labels:\n",
    "    print(f\"Warning: Test dataset contains labels not present in training: {missing_labels}\")\n",
    "else:\n",
    "    print(\"Label encoding is consistent between training and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb3f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing data...\n",
      "Normalization completed.\n"
     ]
    }
   ],
   "source": [
    "# Normalize Features\n",
    "print(\"Normalizing data...\")\n",
    "label_column = 'Label'\n",
    "\n",
    "train_scaled = train_df.copy()\n",
    "test_scaled = test_df.copy()\n",
    "\n",
    "train_features = train_df.loc[:, train_df.columns != label_column]\n",
    "test_features = test_df.loc[:, test_df.columns != label_column]\n",
    "\n",
    "train_scaled.loc[:, train_df.columns != label_column] = normalize(train_features, axis=0, norm='l2')\n",
    "test_scaled.loc[:, test_df.columns != label_column] = normalize(test_features, axis=0, norm='l2')\n",
    "\n",
    "print(\"Normalization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14cc734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE to training data...\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 502130 samples\n",
      "  Class 1: 502130 samples\n",
      "  Class 2: 502130 samples\n",
      "  Class 3: 502130 samples\n",
      "  Class 4: 502130 samples\n",
      "  Class 5: 502130 samples\n",
      "  Class 6: 502130 samples\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE\n",
    "\n",
    "print(\"Applying SMOTE to training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "train_features = train_scaled.drop(columns=label_column)\n",
    "train_labels = train_scaled[label_column]\n",
    "\n",
    "train_features_oversampled, train_labels_oversampled = smote.fit_resample(train_features, train_labels)\n",
    "train_oversampled = pd.DataFrame(train_features_oversampled, columns=train_features.columns)\n",
    "train_oversampled[label_column] = train_labels_oversampled.values\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(train_labels_oversampled, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"  Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d67fba",
   "metadata": {},
   "source": [
    "## Data Export\n",
    "\n",
    "The processed datasets are exported as CSV files to ensure they can be reused for further analysis or model training without needing to repeat the data preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7787982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "All datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "concatenated_df.to_csv('processed_datasets_new/data_with_timestamps_cleaned.csv', index=False)\n",
    "train_scaled.to_csv('processed_datasets_new/training_data_scaled.csv', index=False)\n",
    "test_scaled.to_csv('processed_datasets_new/testing_data_scaled.csv', index=False)\n",
    "train_oversampled.to_csv('processed_datasets_new/training_data_scaled_smote.csv', index=False)\n",
    "print(\"All datasets saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
